{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0.0, max_tokens=5000):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "    token_dict = {\n",
    "        'prompt_tokens': response['usage']['prompt_tokens'],\n",
    "        'completion_tokens': response['usage']['completion_tokens'],\n",
    "        'total_tokens': response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "\n",
    "file_path = (r'C:\\Users\\Sergio\\Desktop\\tws-workspace\\closeAI\\Examples\\content_generation\\content\\nutrition\\Manual-nutricion-dietetica-CARBAJAL.pdf')\n",
    "\n",
    "def clean_pdf_text(file_path):\n",
    "    # Open the PDF file in read-binary mode\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        # Initialize a PDF file reader object\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        # Initialize an empty string to hold the extracted text\n",
    "        text = \"\"\n",
    "\n",
    "        # Loop over all the pages in the PDF (assuming the PDF text is to be read page by page)\n",
    "        for page_number in range(len(reader.pages)):\n",
    "            # Extract the text from the current page\n",
    "            page_text = reader.pages[page_number].extract_text()\n",
    "            \n",
    "            # Add the page text to the main text string\n",
    "            text += \"\\n\" + page_text\n",
    "\n",
    "    # Define patterns for lines to be removed\n",
    "    patterns = [\n",
    "        r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',  # URLs\n",
    "        r'\\bEnero\\b|\\bFebrero\\b|\\bMarzo\\b|\\bAbril\\b|\\bMayo\\b|\\bJunio\\b|\\bJulio\\b|\\bAgosto\\b|\\bSeptiembre\\b|\\bOctubre\\b|\\bNoviembre\\b|\\bDiciembre\\b',  # Months in Spanish\n",
    "        r'\\bÍndice\\b',  # Index\n",
    "        r'^\\d+\\.$',  # Lines that start with a number and end with a dot, which are likely index entries\n",
    "        r'^\\s*\\uf0b7',  # Lines that start with a bullet point (·), which are likely sub-index entries\n",
    "         r'\\b\\b',  # Lines that start with a (o), which are likely sub-index entries\n",
    "        r'^\\d+$',  # Lines that contain only a number, which are likely page numbers\n",
    "        r'\\.{2,}',  # Two or more dots in a row, which are likely from index entries\n",
    "        r'\\s*[\\.0-9]+\\s*$'  # Trailing dots and numbers\n",
    "    ]\n",
    "\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Remove duplicate lines\n",
    "    lines = list(dict.fromkeys(lines))\n",
    "\n",
    "    # Initialize a list to hold cleaned lines\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        # If the line matches any of the patterns, skip it\n",
    "        if any(re.search(pattern, line) for pattern in patterns):\n",
    "            continue\n",
    "\n",
    "        # If the line is empty, skip it\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        # If we reach here, the line is clean, so add it to the list\n",
    "        cleaned_lines.append(line.strip())\n",
    "\n",
    "    # Join the cleaned lines back into a single string\n",
    "    cleaned_text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    # Replace ellipsis (…) with a single space\n",
    "    cleaned_text = re.sub(r'\\…', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "cleaned_text = clean_pdf_text(file_path)\n",
    "print(cleaned_text[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "You are an exceptional topics test generator. I am going to provide you with assessable titles. Follow these steps to answer.\n",
    "\n",
    "Step 0:#### Basic parameters for future steps:\n",
    "Language: Spanish\n",
    "\n",
    "Step 1:#### Conduct in-depth research on each assessable title. This should include the definition, key concepts, and primary subjects \\\n",
    "as if they were a comprehensive university course. Do not output any information in this stage; it will be used in the following steps.\n",
    "\n",
    "Step 2: #### Read all the info  from the pdfs provided and prioritise the information in it and use in a complemmentary way the information you have in your knowledge. You will see that in the info extracted from the pdf there are a lot of characters that are not letters, delete that information.\n",
    "\n",
    "Step 3:#### Based on your research, develop broad, overarching 10 short topics for each assessable title. It´s better if topics try to evaluate general and basic concepts of the assessable.\\\n",
    "Each topic should cover a large portion of the assessable's material and allow for a holistic evaluation of a candidate's understanding. Take into consideration Step 0. Develop 10 topics for each assessable title. \\\n",
    "The topics develop should be theoretical and advance basic concepts.\n",
    "When choosing the 10 topics, bear in mind the following scoring criteria, show me each score, each rated out of 100 and showing the scores:\n",
    "\n",
    "Relevance: The topic should be directly related to the title and description of the assessable. \n",
    "\n",
    "General: The topic should be general and not an speciality, general concepts and advance basic notions. This one is the most important metric, \\\n",
    "generate topics that are very general and broad in the subject and do not focus in a specific syllabus.\n",
    "\n",
    "Breadth: The topic should be broad enough to encompass a wide range of subtopics within the assessable. \n",
    "\n",
    "Clarity: The topic should be clear and understandable.\n",
    "\n",
    "Evaluative Effectiveness: The topic's capability to serve as a robust assessment tool for a professional's comprehension of the general concepts within the title.\n",
    "\n",
    "Step 3:#### Review the words in the topics and in case the name of the assessable title appears at the begginingof the topic, rewrite the topic like this: \\\n",
    "For example, title: Nutricion; topic: Nutricion and calidad alimentaria; Result: Calidad alimentaria en nutricion\n",
    "\n",
    "Step 4:#### Rank the topics in descending order, based on their average score across all five scoring criteria. List these topics along with their scores.\n",
    "\n",
    "Step 5.1:#### Finally, select the top 5 topics that collectively cover the majority of the assessable's content and will allow for a comprehensive and correct evaluation \\\n",
    "of a candidate's knowledge. These collective topics chosen should be broad, covering multiple key areas within the assessable. They shouldnt be an speciality inside of the title, \\\n",
    "they must be general and broad topics of the title any professional will know. Topics chosen must not be similar, so the chosen as a group will cover the mayority of the assessable syllabus.\n",
    "\n",
    "Step 5.2: #### Develop groups, the groups selected must collectively cover the majority of the assessable's syllabus and will allow for a comprehensive and correct evaluation \\\n",
    "of a candidate's knowledge. These groups must not have similar topics so we can make a correct and complete evaluation of the candidate´s knowledge in the assessable syllabus. \\\n",
    "For each title, develop 2 groups that contain 5 titles each.\n",
    "\n",
    "Use the following format:\n",
    "Step 1:#### <step 1 reasoning>\n",
    "Step 2:#### <step 2 reasoning>\n",
    "Step 3:#### <step 3 reasoning>\n",
    "Step 4:#### <step 4 reasoning>\n",
    "Step 5.1:#### <final result 1>\n",
    "Step 5.2:#### <final result 2>\n",
    "\n",
    "\n",
    "Make sure to include {delimiter} to separate every step. All the information of your development and the result has to be in Spanish.\n",
    "\n",
    "The assessable title is Nutricion\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-gpt-3.5-turbo in organization org-9DC7S6nQuUj9Zu0TFYMqsshL on tokens per min. Limit: 90000 / min. Current: 1 / min. Contact us through our help center at help.openai.com if you continue to have issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m user_message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m [\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mEnfermería\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNutrición\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}] \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m messages \u001b[39m=\u001b[39m  [  \n\u001b[0;32m      4\u001b[0m {\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      5\u001b[0m  \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: system_message},  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m  \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m{\u001b[39;00mdelimiter\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00muser_message\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mdelimiter\u001b[39m}\u001b[39;00m\u001b[39m\"\"\"\u001b[39m},  \n\u001b[0;32m     10\u001b[0m ] \n\u001b[1;32m---> 12\u001b[0m response, token_dict \u001b[39m=\u001b[39m get_completion_and_token_count(messages)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mget_completion_and_token_count\u001b[1;34m(messages, model, temperature, max_tokens)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_completion_and_token_count\u001b[39m(messages, \n\u001b[0;32m      2\u001b[0m                                  model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      3\u001b[0m                                  temperature\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, max_tokens\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      5\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      6\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[0;32m      7\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature, \n\u001b[0;32m      8\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens, \n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     11\u001b[0m     content \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m     token_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mprompt_tokens\u001b[39m\u001b[39m'\u001b[39m: response[\u001b[39m'\u001b[39m\u001b[39musage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mprompt_tokens\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     16\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcompletion_tokens\u001b[39m\u001b[39m'\u001b[39m: response[\u001b[39m'\u001b[39m\u001b[39musage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcompletion_tokens\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     17\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtotal_tokens\u001b[39m\u001b[39m'\u001b[39m: response[\u001b[39m'\u001b[39m\u001b[39musage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtotal_tokens\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     18\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Sergio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-9DC7S6nQuUj9Zu0TFYMqsshL on tokens per min. Limit: 90000 / min. Current: 1 / min. Contact us through our help center at help.openai.com if you continue to have issues."
     ]
    }
   ],
   "source": [
    "user_message = \" [{'title': 'Enfermería', 'title':'Nutrición'}] \"\n",
    "\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},  \n",
    " {'role': 'assistant',\n",
    "  'content': cleaned_text},\n",
    "{'role':'user', \n",
    " 'content': f\"\"\"{delimiter}{user_message}{delimiter}\"\"\"},  \n",
    "] \n",
    "\n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
