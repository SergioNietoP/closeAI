{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import ast\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "directory = r'C:\\Users\\Sergio\\Desktop\\TWS\\tws-workspace\\closeAI\\Examples\\content_generation\\content\\machine_learning'\n",
    "all_content = []\n",
    "\n",
    "# Get all PDF files in the directory\n",
    "pdf_files = glob.glob(os.path.join(directory, '*.pdf'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"\"\n",
    "\n",
    "def get_completion_and_token_count(messages, \n",
    "                                 model=\"gpt-3.5-turbo-16k\", \n",
    "                                 temperature=0.0, max_tokens=5000):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "    token_dict = {\n",
    "        'prompt_tokens': response['usage']['prompt_tokens'],\n",
    "        'completion_tokens': response['usage']['completion_tokens'],\n",
    "        'total_tokens': response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_substring(results, substrings):\n",
    "    # For each substring\n",
    "    for sub in substrings:\n",
    "        # Store indices where the substring is found\n",
    "        found_indices = []\n",
    "\n",
    "        # For each result\n",
    "        for index, sub_results in results:\n",
    "            # For each substring result\n",
    "            for sub_res, found in sub_results:\n",
    "                # If the current substring matches and it was found\n",
    "                if sub_res == sub and found:\n",
    "                    # Store the index\n",
    "                    found_indices.append(index)\n",
    "        \n",
    "        # If the substring was found at least once\n",
    "        if found_indices:\n",
    "            # Return the substring and the indices where it was found\n",
    "            return sub, found_indices\n",
    "\n",
    "    # If none of the substrings were found\n",
    "    return None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_topics(pdf_file):\n",
    "    text_content = []\n",
    "    print(os.path.basename(pdf_file))\n",
    "\n",
    "    # Open the pdf file\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        # Initialize a pdf file reader\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        # Loop through each page\n",
    "        for i in range(len(reader.pages)):\n",
    "            # Get the text content of the page\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "\n",
    "            # Add the text to the list\n",
    "            text_content.append(text)\n",
    "\n",
    "    substrings = ['contents', 'indice', 'index', 'índice', 'table of contents']\n",
    "\n",
    "    text_content_first = text_content[:8]\n",
    "\n",
    "    results = list(filter(lambda x: any(sub_res[1] for sub_res in x[1]), enumerate(map(lambda s: [(sub, s.lower().find(sub) != -1) for sub in substrings], text_content_first))))\n",
    "\n",
    "    # Specify the substrings in the order you want to search for them\n",
    "    substrings = [\"table of contents\", \"indice\", \"index\", \"índice\", \"contents\"]\n",
    "\n",
    "    # Call the function with your results and substrings\n",
    "    found_substring, found_indices = find_substring(results, substrings)\n",
    "\n",
    "    print(found_substring)\n",
    "    print(found_indices)\n",
    "\n",
    "    index_location = found_indices[:1]\n",
    "    index_location = int(index_location[0])\n",
    "\n",
    "    print(text_content[index_location])\n",
    "\n",
    "\n",
    "    pattern = re.compile(r\"^\\d+\\.\\s+.+\", re.MULTILINE)\n",
    "\n",
    "    all_matching_substrings = []\n",
    "\n",
    "    matching_dict = {}\n",
    "\n",
    "    for i in range(index_location, index_location + 3):\n",
    "        matching_substrings = pattern.findall(text_content[i])\n",
    "\n",
    "        for substring in matching_substrings:\n",
    "            # Use a regular expression to separate the topic name and page number\n",
    "            match = re.match(r'^(\\d+)\\.\\s+(.+?)(?:\\s+(\\d+))?\\s*$', substring)\n",
    "            if match:\n",
    "                # Store the topic name and page number in the dictionary\n",
    "                matching_dict[match.group(2)] = match.group(3) if match.group(3) else None\n",
    "\n",
    "    \n",
    "    if not all_matching_substrings:\n",
    "        print(\"OTHEEER\")\n",
    "        # Define a new pattern to match the other format\n",
    "        pattern = re.compile(r\"^(\\d+(?:\\.\\d+)*)\\s+(.+)\\s+(\\d+)$\", re.MULTILINE)\n",
    "\n",
    "        for i in range(index_location, index_location + 3):\n",
    "            matching_substrings = pattern.findall(text_content[i])\n",
    "\n",
    "            # Initialize a dictionary to store the topic name and page number\n",
    "\n",
    "            for substring in matching_substrings:\n",
    "                # 'substring' here is already a tuple because re.findall was used\n",
    "                # So we can directly access its elements\n",
    "                index, topic_name, page_number = substring\n",
    "                \n",
    "                # Store the topic name and page number in the dictionary\n",
    "                matching_dict[f\"{index} {topic_name}\"] = page_number\n",
    "\n",
    "            \n",
    "    if matching_dict:\n",
    "            all_matching_substrings.append(matching_dict)\n",
    "\n",
    "    print(len(str(all_matching_substrings)))\n",
    "\n",
    "    # Check the length of all_matching_substrings\n",
    "    if len(str(all_matching_substrings)) > 1500:\n",
    "        # Split the dictionary into two\n",
    "        matching_dict_iter = iter(matching_dict.items())\n",
    "        matching_dict_1 = dict(itertools.islice(matching_dict_iter, len(matching_dict)//2))\n",
    "        matching_dict_2 = dict(matching_dict_iter)\n",
    "\n",
    "        # Replace the original matching_dict in all_matching_substrings with the two new dictionaries\n",
    "        all_matching_substrings = [matching_dict_1, matching_dict_2]\n",
    "\n",
    "    system_message_clean = f\"\"\" \n",
    "    You are going to receive an array of dictionaries. The keys are not cleaned and not clear. \\n\n",
    "    The array will be between triple backstrick (```). \\n\n",
    "    Output the same array of dictionaries with keys strings cleaned and its number in case it has, and its respective dictionary value. \\n\n",
    "    Just output the object asked for. ```{all_matching_substrings}``` \"\"\"\n",
    "\n",
    "    if(len(all_matching_substrings) > 1):\n",
    "        for i in all_matching_substrings:\n",
    "            print(i)\n",
    "            system_message_clean = f\"\"\" \n",
    "                    You are going to receive an array of dictionaries. The keys are not cleaned and not clear. \\n\n",
    "                    The array will be between triple backstrick (```). \\n\n",
    "                    Output the same array of dictionaries with keys strings cleaned and its number in case it has, and its respective dictionary value. \\n\n",
    "                    Just output the object asked for. ```{i}``` \"\"\"\n",
    "            messages =  [  \n",
    "            {'role':'system', \n",
    "            'content': system_message_clean},  \n",
    "            ] \n",
    "            response, token_dict = get_completion_and_token_count(messages)\n",
    "            print(response)\n",
    "            print(token_dict)\n",
    "\n",
    "            # Convert the string representation of the response into an actual list of dictionaries\n",
    "            response = ast.literal_eval(response)\n",
    "\n",
    "            all_content.append({\"file_name\": os.path.basename(pdf_file), \"matching_substrings\": response})\n",
    "    else:\n",
    "        messages =  [  \n",
    "        {'role':'system', \n",
    "        'content': system_message_clean},  \n",
    "        ] \n",
    "        response, token_dict = get_completion_and_token_count(messages)\n",
    "        print(response)\n",
    "        print(token_dict)\n",
    "\n",
    "        # Convert the string representation of the response into an actual list of dictionaries\n",
    "        response = ast.literal_eval(response)\n",
    "\n",
    "        all_content.append({\"file_name\": os.path.basename(pdf_file), \"matching_substrings\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLBOOK.pdf\n",
      "contents\n",
      "[2]\n",
      "Contents\n",
      "1 Preliminaries 1\n",
      "1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n",
      "1.1.1 What is Machine Learning? . . . . . . . . . . . . . . . . . 1\n",
      "1.1.2 Wellsprings of Machine Learning . . . . . . . . . . . . . . 3\n",
      "1.1.3 Varieties of Machine Learning . . . . . . . . . . . . . . . . 4\n",
      "1.2 Learning Input-Output Functions . . . . . . . . . . . . . . . . . . 5\n",
      "1.2.1 Types of Learning . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "1.2.2 Input Vectors . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "1.2.3 Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "1.2.4 Training Regimes . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "1.2.5 Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "1.2.6 Performance Evaluation . . . . . . . . . . . . . . . . . . . 9\n",
      "1.3 Learning Requires Bias . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "1.4 Sample Applications . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
      "1.5 Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
      "1.6 Bibliographical and Historical Remarks . . . . . . . . . . . . . . 13\n",
      "2 Boolean Functions 15\n",
      "2.1 Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "2.1.1 Boolean Algebra . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "2.1.2 Diagrammatic Representations . . . . . . . . . . . . . . . 16\n",
      "2.2 Classes of Boolean Functions . . . . . . . . . . . . . . . . . . . . 17\n",
      "2.2.1 Terms and Clauses . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "2.2.2 DNF Functions . . . . . . . . . . . . . . . . . . . . . . . . 18\n",
      "2.2.3 CNF Functions . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "2.2.4 Decision Lists . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "2.2.5 Symmetric and Voting Functions . . . . . . . . . . . . . . 23\n",
      "2.2.6 Linearly Separable Functions . . . . . . . . . . . . . . . . 23\n",
      "2.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n",
      "2.4 Bibliographical and Historical Remarks . . . . . . . . . . . . . . 25\n",
      "iii\n",
      "OTHEEER\n",
      "7797\n",
      "{'1 Preliminaries': '1', '1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '1', '1.1.1 What is Machine Learning? . . . . . . . . . . . . . . . . .': '1', '1.1.2 Wellsprings of Machine Learning . . . . . . . . . . . . . .': '3', '1.1.3 Varieties of Machine Learning . . . . . . . . . . . . . . . .': '4', '1.2 Learning Input-Output Functions . . . . . . . . . . . . . . . . . .': '5', '1.2.1 Types of Learning . . . . . . . . . . . . . . . . . . . . . .': '5', '1.2.2 Input Vectors . . . . . . . . . . . . . . . . . . . . . . . . .': '7', '1.2.3 Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '8', '1.2.4 Training Regimes . . . . . . . . . . . . . . . . . . . . . . .': '8', '1.2.5 Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '9', '1.2.6 Performance Evaluation . . . . . . . . . . . . . . . . . . .': '9', '1.3 Learning Requires Bias . . . . . . . . . . . . . . . . . . . . . . . .': '9', '1.4 Sample Applications . . . . . . . . . . . . . . . . . . . . . . . . .': '11', '1.5 Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '13', '1.6 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '13', '2 Boolean Functions': '15', '2.1 Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '15', '2.1.1 Boolean Algebra . . . . . . . . . . . . . . . . . . . . . . .': '15', '2.1.2 Diagrammatic Representations . . . . . . . . . . . . . . .': '16', '2.2 Classes of Boolean Functions . . . . . . . . . . . . . . . . . . . .': '17', '2.2.1 Terms and Clauses . . . . . . . . . . . . . . . . . . . . . .': '17', '2.2.2 DNF Functions . . . . . . . . . . . . . . . . . . . . . . . .': '18', '2.2.3 CNF Functions . . . . . . . . . . . . . . . . . . . . . . . .': '21', '2.2.4 Decision Lists . . . . . . . . . . . . . . . . . . . . . . . . .': '22', '2.2.5 Symmetric and Voting Functions . . . . . . . . . . . . . .': '23', '2.2.6 Linearly Separable Functions . . . . . . . . . . . . . . . .': '23', '2.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '24', '2.4 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '25', '3 Using Version Spaces for Learning': '27', '3.1 Version Spaces and Mistake Bounds . . . . . . . . . . . . . . . .': '27', '3.2 Version Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '29', '3.3 Learning as Search of a Version Space . . . . . . . . . . . . . . .': '32', '3.4 The Candidate Elimination Method . . . . . . . . . . . . . . . .': '32', '3.5 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '34', '4 Neural Networks': '35', '4.1 Threshold Logic Units . . . . . . . . . . . . . . . . . . . . . . . .': '35', '4.1.1 De\\x0cnitions and Geometry . . . . . . . . . . . . . . . . . .': '35', '4.1.2 Special Cases of Linearly Separable Functions . . . . . . .': '37', '4.1.3 Error-Correction Training of a TLU . . . . . . . . . . . .': '38', '4.1.4 Weight Space . . . . . . . . . . . . . . . . . . . . . . . . .': '40', '4.1.5 The Widrow-Ho\\x0b Procedure . . . . . . . . . . . . . . . . .': '42', '4.1.6 Training a TLU on Non-Linearly-Separable Training Sets': '44', '4.2 Linear Machines . . . . . . . . . . . . . . . . . . . . . . . . . . .': '44', '4.3 Networks of TLUs . . . . . . . . . . . . . . . . . . . . . . . . . .': '46', '4.3.1 Motivation and Examples . . . . . . . . . . . . . . . . . .': '46', '4.3.2 Madalines . . . . . . . . . . . . . . . . . . . . . . . . . . .': '49', '4.3.3 Piecewise Linear Machines . . . . . . . . . . . . . . . . . .': '50', '4.3.4 Cascade Networks . . . . . . . . . . . . . . . . . . . . . .': '51', '4.4 Training Feedforward Networks by Backpropagation . . . . . . .': '52', '4.4.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '52', '4.4.2 The Backpropagation Method . . . . . . . . . . . . . . . .': '53'}\n",
      "{\n",
      "  \"1 Preliminaries\": \"1\",\n",
      "  \"1.1 Introduction\": \"1\",\n",
      "  \"1.1.1 What is Machine Learning?\": \"1\",\n",
      "  \"1.1.2 Wellsprings of Machine Learning\": \"3\",\n",
      "  \"1.1.3 Varieties of Machine Learning\": \"4\",\n",
      "  \"1.2 Learning Input-Output Functions\": \"5\",\n",
      "  \"1.2.1 Types of Learning\": \"5\",\n",
      "  \"1.2.2 Input Vectors\": \"7\",\n",
      "  \"1.2.3 Outputs\": \"8\",\n",
      "  \"1.2.4 Training Regimes\": \"8\",\n",
      "  \"1.2.5 Noise\": \"9\",\n",
      "  \"1.2.6 Performance Evaluation\": \"9\",\n",
      "  \"1.3 Learning Requires Bias\": \"9\",\n",
      "  \"1.4 Sample Applications\": \"11\",\n",
      "  \"1.5 Sources\": \"13\",\n",
      "  \"1.6 Bibliographical and Historical Remarks\": \"13\",\n",
      "  \"2 Boolean Functions\": \"15\",\n",
      "  \"2.1 Representation\": \"15\",\n",
      "  \"2.1.1 Boolean Algebra\": \"15\",\n",
      "  \"2.1.2 Diagrammatic Representations\": \"16\",\n",
      "  \"2.2 Classes of Boolean Functions\": \"17\",\n",
      "  \"2.2.1 Terms and Clauses\": \"17\",\n",
      "  \"2.2.2 DNF Functions\": \"18\",\n",
      "  \"2.2.3 CNF Functions\": \"21\",\n",
      "  \"2.2.4 Decision Lists\": \"22\",\n",
      "  \"2.2.5 Symmetric and Voting Functions\": \"23\",\n",
      "  \"2.2.6 Linearly Separable Functions\": \"23\",\n",
      "  \"2.3 Summary\": \"24\",\n",
      "  \"2.4 Bibliographical and Historical Remarks\": \"25\",\n",
      "  \"3 Using Version Spaces for Learning\": \"27\",\n",
      "  \"3.1 Version Spaces and Mistake Bounds\": \"27\",\n",
      "  \"3.2 Version Graphs\": \"29\",\n",
      "  \"3.3 Learning as Search of a Version Space\": \"32\",\n",
      "  \"3.4 The Candidate Elimination Method\": \"32\",\n",
      "  \"3.5 Bibliographical and Historical Remarks\": \"34\",\n",
      "  \"4 Neural Networks\": \"35\",\n",
      "  \"4.1 Threshold Logic Units\": \"35\",\n",
      "  \"4.1.1 Definitions and Geometry\": \"35\",\n",
      "  \"4.1.2 Special Cases of Linearly Separable Functions\": \"37\",\n",
      "  \"4.1.3 Error-Correction Training of a TLU\": \"38\",\n",
      "  \"4.1.4 Weight Space\": \"40\",\n",
      "  \"4.1.5 The Widrow-Hoff Procedure\": \"42\",\n",
      "  \"4.1.6 Training a TLU on Non-Linearly-Separable Training Sets\": \"44\",\n",
      "  \"4.2 Linear Machines\": \"44\",\n",
      "  \"4.3 Networks of TLUs\": \"46\",\n",
      "  \"4.3.1 Motivation and Examples\": \"46\",\n",
      "  \"4.3.2 Madalines\": \"49\",\n",
      "  \"4.3.3 Piecewise Linear Machines\": \"50\",\n",
      "  \"4.3.4 Cascade Networks\": \"51\",\n",
      "  \"4.4 Training Feedforward Networks by Backpropagation\": \"52\",\n",
      "  \"4.4.1 Notation\": \"52\",\n",
      "  \"4.4.2 The Backpropagation Method\": \"53\"\n",
      "}\n",
      "{'prompt_tokens': 1729, 'completion_tokens': 722, 'total_tokens': 2451}\n",
      "{'4.4.3 Computing Weight Changes in the Final Layer . . . . . .': '56', '4.4.4 Computing Changes to the Weights in Intermediate Layers': '58', '4.4.5 Variations on Backprop . . . . . . . . . . . . . . . . . . .': '59', '4.4.6 An Application: Steering a Van . . . . . . . . . . . . . . .': '60', '4.5 Synergies Between Neural Network and Knowledge-Based Methods': '61', '4.6 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '61', '5 Statistical Learning': '63', '5.1 Using Statistical Decision Theory . . . . . . . . . . . . . . . . . .': '63', '5.1.1 Background and General Method . . . . . . . . . . . . . .': '63', '5.1.2 Gaussian (or Normal) Distributions . . . . . . . . . . . .': '65', '5.1.3 Conditionally Independent Binary Components . . . . . .': '68', '5.2 Learning Belief Networks . . . . . . . . . . . . . . . . . . . . . .': '70', '5.3 Nearest-Neighbor Methods . . . . . . . . . . . . . . . . . . . . . .': '70', '5.4 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '72', '6 Decision Trees': '73', '6.1 De\\x0cnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '73', '6.2 Supervised Learning of Univariate Decision Trees . . . . . . . . .': '74', '6.2.1 Selecting the Type of Test . . . . . . . . . . . . . . . . . .': '75', '6.2.2 Using Uncertainty Reduction to Select Tests . . . . . . .': '75', '6.2.3 Non-Binary Attributes . . . . . . . . . . . . . . . . . . . .': '79', '6.3 Networks Equivalent to Decision Trees . . . . . . . . . . . . . . .': '79', '6.4 Over\\x0ctting and Evaluation . . . . . . . . . . . . . . . . . . . . .': '80', '6.4.1 Over\\x0ctting . . . . . . . . . . . . . . . . . . . . . . . . . .': '80', '6.4.2 Validation Methods . . . . . . . . . . . . . . . . . . . . .': '81', '6.4.3 Avoiding Over\\x0ctting in Decision Trees . . . . . . . . . . .': '82', '6.4.4 Minimum-Description Length Methods . . . . . . . . . . .': '83', '6.4.5 Noise in Data . . . . . . . . . . . . . . . . . . . . . . . . .': '84', '6.5 The Problem of Replicated Subtrees . . . . . . . . . . . . . . . .': '84', '6.6 The Problem of Missing Attributes . . . . . . . . . . . . . . . . .': '86', '6.7 Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '86', '6.8 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '87', '7 Inductive Logic Programming': '89', '7.1 Notation and De\\x0cnitions . . . . . . . . . . . . . . . . . . . . . . .': '90', '7.2 A Generic ILP Algorithm . . . . . . . . . . . . . . . . . . . . . .': '91', '7.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '94', '7.4 Inducing Recursive Programs . . . . . . . . . . . . . . . . . . . .': '98', '7.5 Choosing Literals to Add . . . . . . . . . . . . . . . . . . . . . .': '100', '7.6 Relationships Between ILP and Decision Tree Induction . . . . .': '101', '7.7 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '104', '8 Computational Learning Theory': '107', '8.1 Notation and Assumptions for PAC Learning Theory . . . . . . .': '107', '8.2 PAC Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '109', '8.2.1 The Fundamental Theorem . . . . . . . . . . . . . . . . .': '109', '8.2.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . .': '111', '8.2.3 Some Properly PAC-Learnable Classes . . . . . . . . . . .': '112', '8.3 The Vapnik-Chervonenkis Dimension . . . . . . . . . . . . . . . .': '113', '8.3.1 Linear Dichotomies . . . . . . . . . . . . . . . . . . . . . .': '113', '8.3.2 Capacity . . . . . . . . . . . . . . . . . . . . . . . . . . .': '115', '8.3.3 A More General Capacity Result . . . . . . . . . . . . . .': '116', '8.3.4 Some Facts and Speculations About the VC Dimension .': '117', '8.4 VC Dimension and PAC Learning . . . . . . . . . . . . . . . . .': '118', '8.5 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '118'}\n",
      "{'4.4.3 Computing Weight Changes in the Final Layer . . . . . .': '56', '4.4.4 Computing Changes to the Weights in Intermediate Layers': '58', '4.4.5 Variations on Backprop . . . . . . . . . . . . . . . . . . .': '59', '4.4.6 An Application: Steering a Van . . . . . . . . . . . . . . .': '60', '4.5 Synergies Between Neural Network and Knowledge-Based Methods': '61', '4.6 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '61', '5 Statistical Learning': '63', '5.1 Using Statistical Decision Theory . . . . . . . . . . . . . . . . . .': '63', '5.1.1 Background and General Method . . . . . . . . . . . . . .': '63', '5.1.2 Gaussian (or Normal) Distributions . . . . . . . . . . . .': '65', '5.1.3 Conditionally Independent Binary Components . . . . . .': '68', '5.2 Learning Belief Networks . . . . . . . . . . . . . . . . . . . . . .': '70', '5.3 Nearest-Neighbor Methods . . . . . . . . . . . . . . . . . . . . . .': '70', '5.4 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '72', '6 Decision Trees': '73', '6.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '73', '6.2 Supervised Learning of Univariate Decision Trees . . . . . . . . .': '74', '6.2.1 Selecting the Type of Test . . . . . . . . . . . . . . . . . .': '75', '6.2.2 Using Uncertainty Reduction to Select Tests . . . . . . .': '75', '6.2.3 Non-Binary Attributes . . . . . . . . . . . . . . . . . . . .': '79', '6.3 Networks Equivalent to Decision Trees . . . . . . . . . . . . . . .': '79', '6.4 Overfitting and Evaluation . . . . . . . . . . . . . . . . . . . . .': '80', '6.4.1 Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . .': '80', '6.4.2 Validation Methods . . . . . . . . . . . . . . . . . . . . .': '81', '6.4.3 Avoiding Overfitting in Decision Trees . . . . . . . . . . .': '82', '6.4.4 Minimum-Description Length Methods . . . . . . . . . . .': '83', '6.4.5 Noise in Data . . . . . . . . . . . . . . . . . . . . . . . . .': '84', '6.5 The Problem of Replicated Subtrees . . . . . . . . . . . . . . . .': '84', '6.6 The Problem of Missing Attributes . . . . . . . . . . . . . . . . .': '86', '6.7 Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '86', '6.8 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '87', '7 Inductive Logic Programming': '89', '7.1 Notation and Definitions . . . . . . . . . . . . . . . . . . . . . . .': '90', '7.2 A Generic ILP Algorithm . . . . . . . . . . . . . . . . . . . . . .': '91', '7.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '94', '7.4 Inducing Recursive Programs . . . . . . . . . . . . . . . . . . . .': '98', '7.5 Choosing Literals to Add . . . . . . . . . . . . . . . . . . . . . .': '100', '7.6 Relationships Between ILP and Decision Tree Induction . . . . .': '101', '7.7 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '104', '8 Computational Learning Theory': '107', '8.1 Notation and Assumptions for PAC Learning Theory . . . . . . .': '107', '8.2 PAC Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '109', '8.2.1 The Fundamental Theorem . . . . . . . . . . . . . . . . .': '109', '8.2.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . .': '111', '8.2.3 Some Properly PAC-Learnable Classes . . . . . . . . . . .': '112', '8.3 The Vapnik-Chervonenkis Dimension . . . . . . . . . . . . . . . .': '113', '8.3.1 Linear Dichotomies . . . . . . . . . . . . . . . . . . . . . .': '113', '8.3.2 Capacity . . . . . . . . . . . . . . . . . . . . . . . . . . .': '115', '8.3.3 A More General Capacity Result . . . . . . . . . . . . . .': '116', '8.3.4 Some Facts and Speculations About the VC Dimension .': '117', '8.4 VC Dimension and PAC Learning . . . . . . . . . . . . . . . . .': '118', '8.5 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '118'}\n",
      "{'prompt_tokens': 1600, 'completion_tokens': 1505, 'total_tokens': 3105}\n",
      "thebook.pdf\n",
      "contents\n",
      "[4, 5, 6, 7]\n",
      "Contents\n",
      "Preface page 1\n",
      "1 Introduction 3\n",
      "1.1 A Taste of Machine Learning 3\n",
      "1.1.1 Applications 3\n",
      "1.1.2 Data 7\n",
      "1.1.3 Problems 9\n",
      "1.2 Probability Theory 12\n",
      "1.2.1 Random Variables 12\n",
      "1.2.2 Distributions 13\n",
      "1.2.3 Mean and Variance 15\n",
      "1.2.4 Marginalization, Independence, Conditioning, and\n",
      "Bayes Rule 16\n",
      "1.3 Basic Algorithms 20\n",
      "1.3.1 Naive Bayes 22\n",
      "1.3.2 Nearest Neighbor Estimators 24\n",
      "1.3.3 A Simple Classi\fer 27\n",
      "1.3.4 Perceptron 29\n",
      "1.3.5 K-Means 32\n",
      "2 Density Estimation 37\n",
      "2.1 Limit Theorems 37\n",
      "2.1.1 Fundamental Laws 38\n",
      "2.1.2 The Characteristic Function 42\n",
      "2.1.3 Tail Bounds 45\n",
      "2.1.4 An Example 48\n",
      "2.2 Parzen Windows 51\n",
      "2.2.1 Discrete Density Estimation 51\n",
      "2.2.2 Smoothing Kernel 52\n",
      "2.2.3 Parameter Estimation 54\n",
      "2.2.4 Silverman's Rule 57\n",
      "2.2.5 Watson-Nadaraya Estimator 59\n",
      "2.3 Exponential Families 60\n",
      "2.3.1 Basics 60\n",
      "v\n",
      "OTHEEER\n",
      "3640\n",
      "{'1 Introduction': '3', '1.1 A Taste of Machine Learning': '3', '1.1.1 Applications': '3', '1.1.2 Data': '7', '1.1.3 Problems': '9', '1.2 Probability Theory': '12', '1.2.1 Random Variables': '12', '1.2.2 Distributions': '13', '1.2.3 Mean and Variance': '15', '1.3 Basic Algorithms': '20', '1.3.1 Naive Bayes': '22', '1.3.2 Nearest Neighbor Estimators': '24', '1.3.3 A Simple Classi\\x0cer': '27', '1.3.4 Perceptron': '29', '1.3.5 K-Means': '32', '2 Density Estimation': '37', '2.1 Limit Theorems': '37', '2.1.1 Fundamental Laws': '38', '2.1.2 The Characteristic Function': '42', '2.1.3 Tail Bounds': '45', '2.1.4 An Example': '48', '2.2 Parzen Windows': '51', '2.2.1 Discrete Density Estimation': '51', '2.2.2 Smoothing Kernel': '52', '2.2.3 Parameter Estimation': '54', \"2.2.4 Silverman's Rule\": '57', '2.2.5 Watson-Nadaraya Estimator': '59', '2.3 Exponential Families': '60', '2.3.1 Basics': '60', '2.3.2 Examples': '62', '2.4 Estimation': '66', '2.4.1 Maximum Likelihood Estimation': '66', '2.4.2 Bias, Variance and Consistency': '68', '2.4.3 A Bayesian Approach': '71', '2.4.4 An Example': '75', '2.5 Sampling': '77', '2.5.1 Inverse Transformation': '78', '2.5.2 Rejection Sampler': '82', '3 Optimization': '91', '3.1 Preliminaries': '91', '3.1.1 Convex Sets': '92', '3.1.2 Convex Functions': '92', '3.1.3 Subgradients': '96', '3.1.4 Strongly Convex Functions': '97', '3.1.5 Convex Functions with Lipschitz Continous Gradient': '98', '3.1.6 Fenchel Duality': '98', '3.1.7 Bregman Divergence': '100', '3.2 Unconstrained Smooth Convex Minimization': '102', '3.2.1 Minimizing a One-Dimensional Convex Function': '102', '3.2.2 Coordinate Descent': '104', '3.2.3 Gradient Descent': '104', '3.2.4 Mirror Descent': '108', '3.2.5 Conjugate Gradient': '111'}\n",
      "{'1 Introduction': '3', '1.1 A Taste of Machine Learning': '3', '1.1.1 Applications': '3', '1.1.2 Data': '7', '1.1.3 Problems': '9', '1.2 Probability Theory': '12', '1.2.1 Random Variables': '12', '1.2.2 Distributions': '13', '1.2.3 Mean and Variance': '15', '1.3 Basic Algorithms': '20', '1.3.1 Naive Bayes': '22', '1.3.2 Nearest Neighbor Estimators': '24', '1.3.3 A Simple Classifier': '27', '1.3.4 Perceptron': '29', '1.3.5 K-Means': '32', '2 Density Estimation': '37', '2.1 Limit Theorems': '37', '2.1.1 Fundamental Laws': '38', '2.1.2 The Characteristic Function': '42', '2.1.3 Tail Bounds': '45', '2.1.4 An Example': '48', '2.2 Parzen Windows': '51', '2.2.1 Discrete Density Estimation': '51', '2.2.2 Smoothing Kernel': '52', '2.2.3 Parameter Estimation': '54', \"2.2.4 Silverman's Rule\": '57', '2.2.5 Watson-Nadaraya Estimator': '59', '2.3 Exponential Families': '60', '2.3.1 Basics': '60', '2.3.2 Examples': '62', '2.4 Estimation': '66', '2.4.1 Maximum Likelihood Estimation': '66', '2.4.2 Bias, Variance and Consistency': '68', '2.4.3 A Bayesian Approach': '71', '2.4.4 An Example': '75', '2.5 Sampling': '77', '2.5.1 Inverse Transformation': '78', '2.5.2 Rejection Sampler': '82', '3 Optimization': '91', '3.1 Preliminaries': '91', '3.1.1 Convex Sets': '92', '3.1.2 Convex Functions': '92', '3.1.3 Subgradients': '96', '3.1.4 Strongly Convex Functions': '97', '3.1.5 Convex Functions with Lipschitz Continous Gradient': '98', '3.1.6 Fenchel Duality': '98', '3.1.7 Bregman Divergence': '100', '3.2 Unconstrained Smooth Convex Minimization': '102', '3.2.1 Minimizing a One-Dimensional Convex Function': '102', '3.2.2 Coordinate Descent': '104', '3.2.3 Gradient Descent': '104', '3.2.4 Mirror Descent': '108', '3.2.5 Conjugate Gradient': '111'}\n",
      "{'prompt_tokens': 765, 'completion_tokens': 680, 'total_tokens': 1445}\n",
      "{'3.2.6 Higher Order Methods': '115', '3.2.7 Bundle Methods': '121', '3.3 Constrained Optimization': '125', '3.3.1 Projection Based Methods': '125', '3.3.2 Lagrange Duality': '127', '3.3.3 Linear and Quadratic Programs': '131', '3.4 Stochastic Optimization': '135', '3.4.1 Stochastic Gradient Descent': '136', '3.5 Nonconvex Optimization': '137', '3.5.1 Concave-Convex Procedure': '137', '3.6 Some Practical Advice': '139', '4 Online Learning and Boosting': '143', '4.1 Halving Algorithm': '143', '4.2 Weighted Majority': '144', '5 Conditional Densities': '149', '5.1 Logistic Regression': '150', '5.2 Regression': '151', '5.2.1 Conditionally Normal Models': '151', '5.2.2 Posterior Distribution': '151', '5.2.3 Heteroscedastic Estimation': '151', '5.3 Multiclass Classi\\x0ccation': '151', '5.3.1 Conditionally Multinomial Models': '151', '5.4 What is a CRF?': '152', '5.4.1 Linear Chain CRFs': '152', '5.4.2 Higher Order CRFs': '152', '5.4.3 Kernelized CRFs': '152', '5.5 Optimization Strategies': '152', '5.5.1 Getting Started': '152', '5.5.2 Optimization Algorithms': '152', '5.5.3 Handling Higher order CRFs': '152', '5.6 Hidden Markov Models': '153', '5.7 Further Reading': '153', '5.7.1 Optimization': '153', '6 Kernels and Function Spaces': '155', '6.1 The Basics': '155', '6.1.1 Examples': '156', '6.2 Kernels': '161', '6.2.1 Feature Maps': '161', '6.2.2 The Kernel Trick': '161', '6.2.3 Examples of Kernels': '161', '6.3 Algorithms': '161', '6.3.1 Kernel Perceptron': '161', '6.3.2 Trivial Classi\\x0cer': '161', '6.3.3 Kernel Principal Component Analysis': '161', '6.4 Reproducing Kernel Hilbert Spaces': '161', '6.4.1 Hilbert Spaces': '163', '6.4.2 Theoretical Properties': '163', '6.4.3 Regularization': '163', '6.5 Banach Spaces': '164', '6.5.1 Properties': '164', '6.5.2 Norms and Convex Sets': '164', '7 Linear Models': '165', '7.1 Support Vector Classi\\x0ccation': '165'}\n",
      "[{'3.2.6 Higher Order Methods': '115', '3.2.7 Bundle Methods': '121', '3.3 Constrained Optimization': '125', '3.3.1 Projection Based Methods': '125', '3.3.2 Lagrange Duality': '127', '3.3.3 Linear and Quadratic Programs': '131', '3.4 Stochastic Optimization': '135', '3.4.1 Stochastic Gradient Descent': '136', '3.5 Nonconvex Optimization': '137', '3.5.1 Concave-Convex Procedure': '137', '3.6 Some Practical Advice': '139', '4 Online Learning and Boosting': '143', '4.1 Halving Algorithm': '143', '4.2 Weighted Majority': '144', '5 Conditional Densities': '149', '5.1 Logistic Regression': '150', '5.2 Regression': '151', '5.2.1 Conditionally Normal Models': '151', '5.2.2 Posterior Distribution': '151', '5.2.3 Heteroscedastic Estimation': '151', '5.3 Multiclass Classification': '151', '5.3.1 Conditionally Multinomial Models': '151', '5.4 What is a CRF?': '152', '5.4.1 Linear Chain CRFs': '152', '5.4.2 Higher Order CRFs': '152', '5.4.3 Kernelized CRFs': '152', '5.5 Optimization Strategies': '152', '5.5.1 Getting Started': '152', '5.5.2 Optimization Algorithms': '152', '5.5.3 Handling Higher order CRFs': '152', '5.6 Hidden Markov Models': '153', '5.7 Further Reading': '153', '5.7.1 Optimization': '153', '6 Kernels and Function Spaces': '155', '6.1 The Basics': '155', '6.1.1 Examples': '156', '6.2 Kernels': '161', '6.2.1 Feature Maps': '161', '6.2.2 The Kernel Trick': '161', '6.2.3 Examples of Kernels': '161', '6.3 Algorithms': '161', '6.3.1 Kernel Perceptron': '161', '6.3.2 Trivial Classifier': '161', '6.3.3 Kernel Principal Component Analysis': '161', '6.4 Reproducing Kernel Hilbert Spaces': '161', '6.4.1 Hilbert Spaces': '163', '6.4.2 Theoretical Properties': '163', '6.4.3 Regularization': '163', '6.5 Banach Spaces': '164', '6.5.1 Properties': '164', '6.5.2 Norms and Convex Sets': '164', '7 Linear Models': '165', '7.1 Support Vector Classification': '165'}]\n",
      "{'prompt_tokens': 749, 'completion_tokens': 656, 'total_tokens': 1405}\n"
     ]
    }
   ],
   "source": [
    "# Process all PDF files\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    get_main_topics(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_name': 'MLBOOK.pdf', 'matching_substrings': {'1 Preliminaries': '1', '1.1 Introduction': '1', '1.1.1 What is Machine Learning?': '1', '1.1.2 Wellsprings of Machine Learning': '3', '1.1.3 Varieties of Machine Learning': '4', '1.2 Learning Input-Output Functions': '5', '1.2.1 Types of Learning': '5', '1.2.2 Input Vectors': '7', '1.2.3 Outputs': '8', '1.2.4 Training Regimes': '8', '1.2.5 Noise': '9', '1.2.6 Performance Evaluation': '9', '1.3 Learning Requires Bias': '9', '1.4 Sample Applications': '11', '1.5 Sources': '13', '1.6 Bibliographical and Historical Remarks': '13', '2 Boolean Functions': '15', '2.1 Representation': '15', '2.1.1 Boolean Algebra': '15', '2.1.2 Diagrammatic Representations': '16', '2.2 Classes of Boolean Functions': '17', '2.2.1 Terms and Clauses': '17', '2.2.2 DNF Functions': '18', '2.2.3 CNF Functions': '21', '2.2.4 Decision Lists': '22', '2.2.5 Symmetric and Voting Functions': '23', '2.2.6 Linearly Separable Functions': '23', '2.3 Summary': '24', '2.4 Bibliographical and Historical Remarks': '25', '3 Using Version Spaces for Learning': '27', '3.1 Version Spaces and Mistake Bounds': '27', '3.2 Version Graphs': '29', '3.3 Learning as Search of a Version Space': '32', '3.4 The Candidate Elimination Method': '32', '3.5 Bibliographical and Historical Remarks': '34', '4 Neural Networks': '35', '4.1 Threshold Logic Units': '35', '4.1.1 Definitions and Geometry': '35', '4.1.2 Special Cases of Linearly Separable Functions': '37', '4.1.3 Error-Correction Training of a TLU': '38', '4.1.4 Weight Space': '40', '4.1.5 The Widrow-Hoff Procedure': '42', '4.1.6 Training a TLU on Non-Linearly-Separable Training Sets': '44', '4.2 Linear Machines': '44', '4.3 Networks of TLUs': '46', '4.3.1 Motivation and Examples': '46', '4.3.2 Madalines': '49', '4.3.3 Piecewise Linear Machines': '50', '4.3.4 Cascade Networks': '51', '4.4 Training Feedforward Networks by Backpropagation': '52', '4.4.1 Notation': '52', '4.4.2 The Backpropagation Method': '53'}}, {'file_name': 'MLBOOK.pdf', 'matching_substrings': {'4.4.3 Computing Weight Changes in the Final Layer . . . . . .': '56', '4.4.4 Computing Changes to the Weights in Intermediate Layers': '58', '4.4.5 Variations on Backprop . . . . . . . . . . . . . . . . . . .': '59', '4.4.6 An Application: Steering a Van . . . . . . . . . . . . . . .': '60', '4.5 Synergies Between Neural Network and Knowledge-Based Methods': '61', '4.6 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '61', '5 Statistical Learning': '63', '5.1 Using Statistical Decision Theory . . . . . . . . . . . . . . . . . .': '63', '5.1.1 Background and General Method . . . . . . . . . . . . . .': '63', '5.1.2 Gaussian (or Normal) Distributions . . . . . . . . . . . .': '65', '5.1.3 Conditionally Independent Binary Components . . . . . .': '68', '5.2 Learning Belief Networks . . . . . . . . . . . . . . . . . . . . . .': '70', '5.3 Nearest-Neighbor Methods . . . . . . . . . . . . . . . . . . . . . .': '70', '5.4 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '72', '6 Decision Trees': '73', '6.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '73', '6.2 Supervised Learning of Univariate Decision Trees . . . . . . . . .': '74', '6.2.1 Selecting the Type of Test . . . . . . . . . . . . . . . . . .': '75', '6.2.2 Using Uncertainty Reduction to Select Tests . . . . . . .': '75', '6.2.3 Non-Binary Attributes . . . . . . . . . . . . . . . . . . . .': '79', '6.3 Networks Equivalent to Decision Trees . . . . . . . . . . . . . . .': '79', '6.4 Overfitting and Evaluation . . . . . . . . . . . . . . . . . . . . .': '80', '6.4.1 Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . .': '80', '6.4.2 Validation Methods . . . . . . . . . . . . . . . . . . . . .': '81', '6.4.3 Avoiding Overfitting in Decision Trees . . . . . . . . . . .': '82', '6.4.4 Minimum-Description Length Methods . . . . . . . . . . .': '83', '6.4.5 Noise in Data . . . . . . . . . . . . . . . . . . . . . . . . .': '84', '6.5 The Problem of Replicated Subtrees . . . . . . . . . . . . . . . .': '84', '6.6 The Problem of Missing Attributes . . . . . . . . . . . . . . . . .': '86', '6.7 Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '86', '6.8 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '87', '7 Inductive Logic Programming': '89', '7.1 Notation and Definitions . . . . . . . . . . . . . . . . . . . . . . .': '90', '7.2 A Generic ILP Algorithm . . . . . . . . . . . . . . . . . . . . . .': '91', '7.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '94', '7.4 Inducing Recursive Programs . . . . . . . . . . . . . . . . . . . .': '98', '7.5 Choosing Literals to Add . . . . . . . . . . . . . . . . . . . . . .': '100', '7.6 Relationships Between ILP and Decision Tree Induction . . . . .': '101', '7.7 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '104', '8 Computational Learning Theory': '107', '8.1 Notation and Assumptions for PAC Learning Theory . . . . . . .': '107', '8.2 PAC Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '109', '8.2.1 The Fundamental Theorem . . . . . . . . . . . . . . . . .': '109', '8.2.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . .': '111', '8.2.3 Some Properly PAC-Learnable Classes . . . . . . . . . . .': '112', '8.3 The Vapnik-Chervonenkis Dimension . . . . . . . . . . . . . . . .': '113', '8.3.1 Linear Dichotomies . . . . . . . . . . . . . . . . . . . . . .': '113', '8.3.2 Capacity . . . . . . . . . . . . . . . . . . . . . . . . . . .': '115', '8.3.3 A More General Capacity Result . . . . . . . . . . . . . .': '116', '8.3.4 Some Facts and Speculations About the VC Dimension .': '117', '8.4 VC Dimension and PAC Learning . . . . . . . . . . . . . . . . .': '118', '8.5 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '118'}}, {'file_name': 'thebook.pdf', 'matching_substrings': {'1 Introduction': '3', '1.1 A Taste of Machine Learning': '3', '1.1.1 Applications': '3', '1.1.2 Data': '7', '1.1.3 Problems': '9', '1.2 Probability Theory': '12', '1.2.1 Random Variables': '12', '1.2.2 Distributions': '13', '1.2.3 Mean and Variance': '15', '1.3 Basic Algorithms': '20', '1.3.1 Naive Bayes': '22', '1.3.2 Nearest Neighbor Estimators': '24', '1.3.3 A Simple Classifier': '27', '1.3.4 Perceptron': '29', '1.3.5 K-Means': '32', '2 Density Estimation': '37', '2.1 Limit Theorems': '37', '2.1.1 Fundamental Laws': '38', '2.1.2 The Characteristic Function': '42', '2.1.3 Tail Bounds': '45', '2.1.4 An Example': '48', '2.2 Parzen Windows': '51', '2.2.1 Discrete Density Estimation': '51', '2.2.2 Smoothing Kernel': '52', '2.2.3 Parameter Estimation': '54', \"2.2.4 Silverman's Rule\": '57', '2.2.5 Watson-Nadaraya Estimator': '59', '2.3 Exponential Families': '60', '2.3.1 Basics': '60', '2.3.2 Examples': '62', '2.4 Estimation': '66', '2.4.1 Maximum Likelihood Estimation': '66', '2.4.2 Bias, Variance and Consistency': '68', '2.4.3 A Bayesian Approach': '71', '2.4.4 An Example': '75', '2.5 Sampling': '77', '2.5.1 Inverse Transformation': '78', '2.5.2 Rejection Sampler': '82', '3 Optimization': '91', '3.1 Preliminaries': '91', '3.1.1 Convex Sets': '92', '3.1.2 Convex Functions': '92', '3.1.3 Subgradients': '96', '3.1.4 Strongly Convex Functions': '97', '3.1.5 Convex Functions with Lipschitz Continous Gradient': '98', '3.1.6 Fenchel Duality': '98', '3.1.7 Bregman Divergence': '100', '3.2 Unconstrained Smooth Convex Minimization': '102', '3.2.1 Minimizing a One-Dimensional Convex Function': '102', '3.2.2 Coordinate Descent': '104', '3.2.3 Gradient Descent': '104', '3.2.4 Mirror Descent': '108', '3.2.5 Conjugate Gradient': '111'}}, {'file_name': 'thebook.pdf', 'matching_substrings': [{'3.2.6 Higher Order Methods': '115', '3.2.7 Bundle Methods': '121', '3.3 Constrained Optimization': '125', '3.3.1 Projection Based Methods': '125', '3.3.2 Lagrange Duality': '127', '3.3.3 Linear and Quadratic Programs': '131', '3.4 Stochastic Optimization': '135', '3.4.1 Stochastic Gradient Descent': '136', '3.5 Nonconvex Optimization': '137', '3.5.1 Concave-Convex Procedure': '137', '3.6 Some Practical Advice': '139', '4 Online Learning and Boosting': '143', '4.1 Halving Algorithm': '143', '4.2 Weighted Majority': '144', '5 Conditional Densities': '149', '5.1 Logistic Regression': '150', '5.2 Regression': '151', '5.2.1 Conditionally Normal Models': '151', '5.2.2 Posterior Distribution': '151', '5.2.3 Heteroscedastic Estimation': '151', '5.3 Multiclass Classification': '151', '5.3.1 Conditionally Multinomial Models': '151', '5.4 What is a CRF?': '152', '5.4.1 Linear Chain CRFs': '152', '5.4.2 Higher Order CRFs': '152', '5.4.3 Kernelized CRFs': '152', '5.5 Optimization Strategies': '152', '5.5.1 Getting Started': '152', '5.5.2 Optimization Algorithms': '152', '5.5.3 Handling Higher order CRFs': '152', '5.6 Hidden Markov Models': '153', '5.7 Further Reading': '153', '5.7.1 Optimization': '153', '6 Kernels and Function Spaces': '155', '6.1 The Basics': '155', '6.1.1 Examples': '156', '6.2 Kernels': '161', '6.2.1 Feature Maps': '161', '6.2.2 The Kernel Trick': '161', '6.2.3 Examples of Kernels': '161', '6.3 Algorithms': '161', '6.3.1 Kernel Perceptron': '161', '6.3.2 Trivial Classifier': '161', '6.3.3 Kernel Principal Component Analysis': '161', '6.4 Reproducing Kernel Hilbert Spaces': '161', '6.4.1 Hilbert Spaces': '163', '6.4.2 Theoretical Properties': '163', '6.4.3 Regularization': '163', '6.5 Banach Spaces': '164', '6.5.1 Properties': '164', '6.5.2 Norms and Convex Sets': '164', '7 Linear Models': '165', '7.1 Support Vector Classification': '165'}]}]\n"
     ]
    }
   ],
   "source": [
    "print(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indices_to_keys(data):\n",
    "    for file_dict in data:\n",
    "        if isinstance(file_dict['matching_substrings'], list):\n",
    "            for sub_dict in file_dict['matching_substrings']:\n",
    "                new_sub_dict = {}\n",
    "                for idx, (key, value) in enumerate(sub_dict.items(), 1):\n",
    "                    # If the key doesn't already start with a numeric pattern, prepend it with the index\n",
    "                    if not re.match(r\"^\\d+(\\.\\d+)*\", key):\n",
    "                        new_sub_dict[f\"{idx}. {key}\"] = value\n",
    "                    else:\n",
    "                        new_sub_dict[key] = value\n",
    "                sub_dict.clear()\n",
    "                sub_dict.update(new_sub_dict)\n",
    "        else:\n",
    "            new_sub_dict = {}\n",
    "            for idx, (key, value) in enumerate(file_dict['matching_substrings'].items(), 1):\n",
    "                if not re.match(r\"^\\d+(\\.\\d+)*\", key):\n",
    "                    new_sub_dict[f\"{idx}. {key}\"] = value\n",
    "                else:\n",
    "                    new_sub_dict[key] = value\n",
    "            file_dict['matching_substrings'].clear()\n",
    "            file_dict['matching_substrings'].update(new_sub_dict)\n",
    "    return data\n",
    "\n",
    "all_content = add_indices_to_keys(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_name': 'MLBOOK.pdf', 'matching_substrings': {'1 Preliminaries': '1', '1.1 Introduction': '1', '1.1.1 What is Machine Learning?': '1', '1.1.2 Wellsprings of Machine Learning': '3', '1.1.3 Varieties of Machine Learning': '4', '1.2 Learning Input-Output Functions': '5', '1.2.1 Types of Learning': '5', '1.2.2 Input Vectors': '7', '1.2.3 Outputs': '8', '1.2.4 Training Regimes': '8', '1.2.5 Noise': '9', '1.2.6 Performance Evaluation': '9', '1.3 Learning Requires Bias': '9', '1.4 Sample Applications': '11', '1.5 Sources': '13', '1.6 Bibliographical and Historical Remarks': '13', '2 Boolean Functions': '15', '2.1 Representation': '15', '2.1.1 Boolean Algebra': '15', '2.1.2 Diagrammatic Representations': '16', '2.2 Classes of Boolean Functions': '17', '2.2.1 Terms and Clauses': '17', '2.2.2 DNF Functions': '18', '2.2.3 CNF Functions': '21', '2.2.4 Decision Lists': '22', '2.2.5 Symmetric and Voting Functions': '23', '2.2.6 Linearly Separable Functions': '23', '2.3 Summary': '24', '2.4 Bibliographical and Historical Remarks': '25', '3 Using Version Spaces for Learning': '27', '3.1 Version Spaces and Mistake Bounds': '27', '3.2 Version Graphs': '29', '3.3 Learning as Search of a Version Space': '32', '3.4 The Candidate Elimination Method': '32', '3.5 Bibliographical and Historical Remarks': '34', '4 Neural Networks': '35', '4.1 Threshold Logic Units': '35', '4.1.1 Definitions and Geometry': '35', '4.1.2 Special Cases of Linearly Separable Functions': '37', '4.1.3 Error-Correction Training of a TLU': '38', '4.1.4 Weight Space': '40', '4.1.5 The Widrow-Hoff Procedure': '42', '4.1.6 Training a TLU on Non-Linearly-Separable Training Sets': '44', '4.2 Linear Machines': '44', '4.3 Networks of TLUs': '46', '4.3.1 Motivation and Examples': '46', '4.3.2 Madalines': '49', '4.3.3 Piecewise Linear Machines': '50', '4.3.4 Cascade Networks': '51', '4.4 Training Feedforward Networks by Backpropagation': '52', '4.4.1 Notation': '52', '4.4.2 The Backpropagation Method': '53'}}, {'file_name': 'MLBOOK.pdf', 'matching_substrings': {'4.4.3 Computing Weight Changes in the Final Layer . . . . . .': '56', '4.4.4 Computing Changes to the Weights in Intermediate Layers': '58', '4.4.5 Variations on Backprop . . . . . . . . . . . . . . . . . . .': '59', '4.4.6 An Application: Steering a Van . . . . . . . . . . . . . . .': '60', '4.5 Synergies Between Neural Network and Knowledge-Based Methods': '61', '4.6 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '61', '5 Statistical Learning': '63', '5.1 Using Statistical Decision Theory . . . . . . . . . . . . . . . . . .': '63', '5.1.1 Background and General Method . . . . . . . . . . . . . .': '63', '5.1.2 Gaussian (or Normal) Distributions . . . . . . . . . . . .': '65', '5.1.3 Conditionally Independent Binary Components . . . . . .': '68', '5.2 Learning Belief Networks . . . . . . . . . . . . . . . . . . . . . .': '70', '5.3 Nearest-Neighbor Methods . . . . . . . . . . . . . . . . . . . . . .': '70', '5.4 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '72', '6 Decision Trees': '73', '6.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '73', '6.2 Supervised Learning of Univariate Decision Trees . . . . . . . . .': '74', '6.2.1 Selecting the Type of Test . . . . . . . . . . . . . . . . . .': '75', '6.2.2 Using Uncertainty Reduction to Select Tests . . . . . . .': '75', '6.2.3 Non-Binary Attributes . . . . . . . . . . . . . . . . . . . .': '79', '6.3 Networks Equivalent to Decision Trees . . . . . . . . . . . . . . .': '79', '6.4 Overfitting and Evaluation . . . . . . . . . . . . . . . . . . . . .': '80', '6.4.1 Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . .': '80', '6.4.2 Validation Methods . . . . . . . . . . . . . . . . . . . . .': '81', '6.4.3 Avoiding Overfitting in Decision Trees . . . . . . . . . . .': '82', '6.4.4 Minimum-Description Length Methods . . . . . . . . . . .': '83', '6.4.5 Noise in Data . . . . . . . . . . . . . . . . . . . . . . . . .': '84', '6.5 The Problem of Replicated Subtrees . . . . . . . . . . . . . . . .': '84', '6.6 The Problem of Missing Attributes . . . . . . . . . . . . . . . . .': '86', '6.7 Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '86', '6.8 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '87', '7 Inductive Logic Programming': '89', '7.1 Notation and Definitions . . . . . . . . . . . . . . . . . . . . . . .': '90', '7.2 A Generic ILP Algorithm . . . . . . . . . . . . . . . . . . . . . .': '91', '7.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '94', '7.4 Inducing Recursive Programs . . . . . . . . . . . . . . . . . . . .': '98', '7.5 Choosing Literals to Add . . . . . . . . . . . . . . . . . . . . . .': '100', '7.6 Relationships Between ILP and Decision Tree Induction . . . . .': '101', '7.7 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '104', '8 Computational Learning Theory': '107', '8.1 Notation and Assumptions for PAC Learning Theory . . . . . . .': '107', '8.2 PAC Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '109', '8.2.1 The Fundamental Theorem . . . . . . . . . . . . . . . . .': '109', '8.2.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . .': '111', '8.2.3 Some Properly PAC-Learnable Classes . . . . . . . . . . .': '112', '8.3 The Vapnik-Chervonenkis Dimension . . . . . . . . . . . . . . . .': '113', '8.3.1 Linear Dichotomies . . . . . . . . . . . . . . . . . . . . . .': '113', '8.3.2 Capacity . . . . . . . . . . . . . . . . . . . . . . . . . . .': '115', '8.3.3 A More General Capacity Result . . . . . . . . . . . . . .': '116', '8.3.4 Some Facts and Speculations About the VC Dimension .': '117', '8.4 VC Dimension and PAC Learning . . . . . . . . . . . . . . . . .': '118', '8.5 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '118'}}, {'file_name': 'thebook.pdf', 'matching_substrings': {'1 Introduction': '3', '1.1 A Taste of Machine Learning': '3', '1.1.1 Applications': '3', '1.1.2 Data': '7', '1.1.3 Problems': '9', '1.2 Probability Theory': '12', '1.2.1 Random Variables': '12', '1.2.2 Distributions': '13', '1.2.3 Mean and Variance': '15', '1.3 Basic Algorithms': '20', '1.3.1 Naive Bayes': '22', '1.3.2 Nearest Neighbor Estimators': '24', '1.3.3 A Simple Classifier': '27', '1.3.4 Perceptron': '29', '1.3.5 K-Means': '32', '2 Density Estimation': '37', '2.1 Limit Theorems': '37', '2.1.1 Fundamental Laws': '38', '2.1.2 The Characteristic Function': '42', '2.1.3 Tail Bounds': '45', '2.1.4 An Example': '48', '2.2 Parzen Windows': '51', '2.2.1 Discrete Density Estimation': '51', '2.2.2 Smoothing Kernel': '52', '2.2.3 Parameter Estimation': '54', \"2.2.4 Silverman's Rule\": '57', '2.2.5 Watson-Nadaraya Estimator': '59', '2.3 Exponential Families': '60', '2.3.1 Basics': '60', '2.3.2 Examples': '62', '2.4 Estimation': '66', '2.4.1 Maximum Likelihood Estimation': '66', '2.4.2 Bias, Variance and Consistency': '68', '2.4.3 A Bayesian Approach': '71', '2.4.4 An Example': '75', '2.5 Sampling': '77', '2.5.1 Inverse Transformation': '78', '2.5.2 Rejection Sampler': '82', '3 Optimization': '91', '3.1 Preliminaries': '91', '3.1.1 Convex Sets': '92', '3.1.2 Convex Functions': '92', '3.1.3 Subgradients': '96', '3.1.4 Strongly Convex Functions': '97', '3.1.5 Convex Functions with Lipschitz Continous Gradient': '98', '3.1.6 Fenchel Duality': '98', '3.1.7 Bregman Divergence': '100', '3.2 Unconstrained Smooth Convex Minimization': '102', '3.2.1 Minimizing a One-Dimensional Convex Function': '102', '3.2.2 Coordinate Descent': '104', '3.2.3 Gradient Descent': '104', '3.2.4 Mirror Descent': '108', '3.2.5 Conjugate Gradient': '111'}}, {'file_name': 'thebook.pdf', 'matching_substrings': [{'3.2.6 Higher Order Methods': '115', '3.2.7 Bundle Methods': '121', '3.3 Constrained Optimization': '125', '3.3.1 Projection Based Methods': '125', '3.3.2 Lagrange Duality': '127', '3.3.3 Linear and Quadratic Programs': '131', '3.4 Stochastic Optimization': '135', '3.4.1 Stochastic Gradient Descent': '136', '3.5 Nonconvex Optimization': '137', '3.5.1 Concave-Convex Procedure': '137', '3.6 Some Practical Advice': '139', '4 Online Learning and Boosting': '143', '4.1 Halving Algorithm': '143', '4.2 Weighted Majority': '144', '5 Conditional Densities': '149', '5.1 Logistic Regression': '150', '5.2 Regression': '151', '5.2.1 Conditionally Normal Models': '151', '5.2.2 Posterior Distribution': '151', '5.2.3 Heteroscedastic Estimation': '151', '5.3 Multiclass Classification': '151', '5.3.1 Conditionally Multinomial Models': '151', '5.4 What is a CRF?': '152', '5.4.1 Linear Chain CRFs': '152', '5.4.2 Higher Order CRFs': '152', '5.4.3 Kernelized CRFs': '152', '5.5 Optimization Strategies': '152', '5.5.1 Getting Started': '152', '5.5.2 Optimization Algorithms': '152', '5.5.3 Handling Higher order CRFs': '152', '5.6 Hidden Markov Models': '153', '5.7 Further Reading': '153', '5.7.1 Optimization': '153', '6 Kernels and Function Spaces': '155', '6.1 The Basics': '155', '6.1.1 Examples': '156', '6.2 Kernels': '161', '6.2.1 Feature Maps': '161', '6.2.2 The Kernel Trick': '161', '6.2.3 Examples of Kernels': '161', '6.3 Algorithms': '161', '6.3.1 Kernel Perceptron': '161', '6.3.2 Trivial Classifier': '161', '6.3.3 Kernel Principal Component Analysis': '161', '6.4 Reproducing Kernel Hilbert Spaces': '161', '6.4.1 Hilbert Spaces': '163', '6.4.2 Theoretical Properties': '163', '6.4.3 Regularization': '163', '6.5 Banach Spaces': '164', '6.5.1 Properties': '164', '6.5.2 Norms and Convex Sets': '164', '7 Linear Models': '165', '7.1 Support Vector Classification': '165'}]}]\n"
     ]
    }
   ],
   "source": [
    "print(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'file_name': 'MLBOOK.pdf',\n",
      "        'matching_substrings': {   '1 Preliminaries': '1',\n",
      "                                   '1.1 Introduction': '1',\n",
      "                                   '1.1.1 What is Machine Learning?': '1',\n",
      "                                   '1.1.2 Wellsprings of Machine Learning': '3',\n",
      "                                   '1.1.3 Varieties of Machine Learning': '4',\n",
      "                                   '1.2 Learning Input-Output Functions': '5',\n",
      "                                   '1.2.1 Types of Learning': '5',\n",
      "                                   '1.2.2 Input Vectors': '7',\n",
      "                                   '1.2.3 Outputs': '8',\n",
      "                                   '1.2.4 Training Regimes': '8',\n",
      "                                   '1.2.5 Noise': '9',\n",
      "                                   '1.2.6 Performance Evaluation': '9',\n",
      "                                   '1.3 Learning Requires Bias': '9',\n",
      "                                   '1.4 Sample Applications': '11',\n",
      "                                   '1.5 Sources': '13',\n",
      "                                   '1.6 Bibliographical and Historical Remarks': '13',\n",
      "                                   '2 Boolean Functions': '15',\n",
      "                                   '2.1 Representation': '15',\n",
      "                                   '2.1.1 Boolean Algebra': '15',\n",
      "                                   '2.1.2 Diagrammatic Representations': '16',\n",
      "                                   '2.2 Classes of Boolean Functions': '17',\n",
      "                                   '2.2.1 Terms and Clauses': '17',\n",
      "                                   '2.2.2 DNF Functions': '18',\n",
      "                                   '2.2.3 CNF Functions': '21',\n",
      "                                   '2.2.4 Decision Lists': '22',\n",
      "                                   '2.2.5 Symmetric and Voting Functions': '23',\n",
      "                                   '2.2.6 Linearly Separable Functions': '23',\n",
      "                                   '2.3 Summary': '24',\n",
      "                                   '2.4 Bibliographical and Historical Remarks': '25',\n",
      "                                   '3 Using Version Spaces for Learning': '27',\n",
      "                                   '3.1 Version Spaces and Mistake Bounds': '27',\n",
      "                                   '3.2 Version Graphs': '29',\n",
      "                                   '3.3 Learning as Search of a Version Space': '32',\n",
      "                                   '3.4 The Candidate Elimination Method': '32',\n",
      "                                   '3.5 Bibliographical and Historical Remarks': '34',\n",
      "                                   '4 Neural Networks': '35',\n",
      "                                   '4.1 Threshold Logic Units': '35',\n",
      "                                   '4.1.1 Definitions and Geometry': '35',\n",
      "                                   '4.1.2 Special Cases of Linearly Separable Functions': '37',\n",
      "                                   '4.1.3 Error-Correction Training of a TLU': '38',\n",
      "                                   '4.1.4 Weight Space': '40',\n",
      "                                   '4.1.5 The Widrow-Hoff Procedure': '42',\n",
      "                                   '4.1.6 Training a TLU on Non-Linearly-Separable Training Sets': '44',\n",
      "                                   '4.2 Linear Machines': '44',\n",
      "                                   '4.3 Networks of TLUs': '46',\n",
      "                                   '4.3.1 Motivation and Examples': '46',\n",
      "                                   '4.3.2 Madalines': '49',\n",
      "                                   '4.3.3 Piecewise Linear Machines': '50',\n",
      "                                   '4.3.4 Cascade Networks': '51',\n",
      "                                   '4.4 Training Feedforward Networks by Backpropagation': '52',\n",
      "                                   '4.4.1 Notation': '52',\n",
      "                                   '4.4.2 The Backpropagation Method': '53'}},\n",
      "    {   'file_name': 'MLBOOK.pdf',\n",
      "        'matching_substrings': {   '4.4.3 Computing Weight Changes in the Final Layer . . . . . .': '56',\n",
      "                                   '4.4.4 Computing Changes to the Weights in Intermediate Layers': '58',\n",
      "                                   '4.4.5 Variations on Backprop . . . . . . . . . . . . . . . . . . .': '59',\n",
      "                                   '4.4.6 An Application: Steering a Van . . . . . . . . . . . . . . .': '60',\n",
      "                                   '4.5 Synergies Between Neural Network and Knowledge-Based Methods': '61',\n",
      "                                   '4.6 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '61',\n",
      "                                   '5 Statistical Learning': '63',\n",
      "                                   '5.1 Using Statistical Decision Theory . . . . . . . . . . . . . . . . . .': '63',\n",
      "                                   '5.1.1 Background and General Method . . . . . . . . . . . . . .': '63',\n",
      "                                   '5.1.2 Gaussian (or Normal) Distributions . . . . . . . . . . . .': '65',\n",
      "                                   '5.1.3 Conditionally Independent Binary Components . . . . . .': '68',\n",
      "                                   '5.2 Learning Belief Networks . . . . . . . . . . . . . . . . . . . . . .': '70',\n",
      "                                   '5.3 Nearest-Neighbor Methods . . . . . . . . . . . . . . . . . . . . . .': '70',\n",
      "                                   '5.4 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '72',\n",
      "                                   '6 Decision Trees': '73',\n",
      "                                   '6.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '73',\n",
      "                                   '6.2 Supervised Learning of Univariate Decision Trees . . . . . . . . .': '74',\n",
      "                                   '6.2.1 Selecting the Type of Test . . . . . . . . . . . . . . . . . .': '75',\n",
      "                                   '6.2.2 Using Uncertainty Reduction to Select Tests . . . . . . .': '75',\n",
      "                                   '6.2.3 Non-Binary Attributes . . . . . . . . . . . . . . . . . . . .': '79',\n",
      "                                   '6.3 Networks Equivalent to Decision Trees . . . . . . . . . . . . . . .': '79',\n",
      "                                   '6.4 Overfitting and Evaluation . . . . . . . . . . . . . . . . . . . . .': '80',\n",
      "                                   '6.4.1 Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . .': '80',\n",
      "                                   '6.4.2 Validation Methods . . . . . . . . . . . . . . . . . . . . .': '81',\n",
      "                                   '6.4.3 Avoiding Overfitting in Decision Trees . . . . . . . . . . .': '82',\n",
      "                                   '6.4.4 Minimum-Description Length Methods . . . . . . . . . . .': '83',\n",
      "                                   '6.4.5 Noise in Data . . . . . . . . . . . . . . . . . . . . . . . . .': '84',\n",
      "                                   '6.5 The Problem of Replicated Subtrees . . . . . . . . . . . . . . . .': '84',\n",
      "                                   '6.6 The Problem of Missing Attributes . . . . . . . . . . . . . . . . .': '86',\n",
      "                                   '6.7 Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '86',\n",
      "                                   '6.8 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '87',\n",
      "                                   '7 Inductive Logic Programming': '89',\n",
      "                                   '7.1 Notation and Definitions . . . . . . . . . . . . . . . . . . . . . . .': '90',\n",
      "                                   '7.2 A Generic ILP Algorithm . . . . . . . . . . . . . . . . . . . . . .': '91',\n",
      "                                   '7.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '94',\n",
      "                                   '7.4 Inducing Recursive Programs . . . . . . . . . . . . . . . . . . . .': '98',\n",
      "                                   '7.5 Choosing Literals to Add . . . . . . . . . . . . . . . . . . . . . .': '100',\n",
      "                                   '7.6 Relationships Between ILP and Decision Tree Induction . . . . .': '101',\n",
      "                                   '7.7 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '104',\n",
      "                                   '8 Computational Learning Theory': '107',\n",
      "                                   '8.1 Notation and Assumptions for PAC Learning Theory . . . . . . .': '107',\n",
      "                                   '8.2 PAC Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .': '109',\n",
      "                                   '8.2.1 The Fundamental Theorem . . . . . . . . . . . . . . . . .': '109',\n",
      "                                   '8.2.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . .': '111',\n",
      "                                   '8.2.3 Some Properly PAC-Learnable Classes . . . . . . . . . . .': '112',\n",
      "                                   '8.3 The Vapnik-Chervonenkis Dimension . . . . . . . . . . . . . . . .': '113',\n",
      "                                   '8.3.1 Linear Dichotomies . . . . . . . . . . . . . . . . . . . . . .': '113',\n",
      "                                   '8.3.2 Capacity . . . . . . . . . . . . . . . . . . . . . . . . . . .': '115',\n",
      "                                   '8.3.3 A More General Capacity Result . . . . . . . . . . . . . .': '116',\n",
      "                                   '8.3.4 Some Facts and Speculations About the VC Dimension .': '117',\n",
      "                                   '8.4 VC Dimension and PAC Learning . . . . . . . . . . . . . . . . .': '118',\n",
      "                                   '8.5 Bibliographical and Historical Remarks . . . . . . . . . . . . . .': '118'}},\n",
      "    {   'file_name': 'thebook.pdf',\n",
      "        'matching_substrings': {   '1 Introduction': '3',\n",
      "                                   '1.1 A Taste of Machine Learning': '3',\n",
      "                                   '1.1.1 Applications': '3',\n",
      "                                   '1.1.2 Data': '7',\n",
      "                                   '1.1.3 Problems': '9',\n",
      "                                   '1.2 Probability Theory': '12',\n",
      "                                   '1.2.1 Random Variables': '12',\n",
      "                                   '1.2.2 Distributions': '13',\n",
      "                                   '1.2.3 Mean and Variance': '15',\n",
      "                                   '1.3 Basic Algorithms': '20',\n",
      "                                   '1.3.1 Naive Bayes': '22',\n",
      "                                   '1.3.2 Nearest Neighbor Estimators': '24',\n",
      "                                   '1.3.3 A Simple Classifier': '27',\n",
      "                                   '1.3.4 Perceptron': '29',\n",
      "                                   '1.3.5 K-Means': '32',\n",
      "                                   '2 Density Estimation': '37',\n",
      "                                   '2.1 Limit Theorems': '37',\n",
      "                                   '2.1.1 Fundamental Laws': '38',\n",
      "                                   '2.1.2 The Characteristic Function': '42',\n",
      "                                   '2.1.3 Tail Bounds': '45',\n",
      "                                   '2.1.4 An Example': '48',\n",
      "                                   '2.2 Parzen Windows': '51',\n",
      "                                   '2.2.1 Discrete Density Estimation': '51',\n",
      "                                   '2.2.2 Smoothing Kernel': '52',\n",
      "                                   '2.2.3 Parameter Estimation': '54',\n",
      "                                   \"2.2.4 Silverman's Rule\": '57',\n",
      "                                   '2.2.5 Watson-Nadaraya Estimator': '59',\n",
      "                                   '2.3 Exponential Families': '60',\n",
      "                                   '2.3.1 Basics': '60',\n",
      "                                   '2.3.2 Examples': '62',\n",
      "                                   '2.4 Estimation': '66',\n",
      "                                   '2.4.1 Maximum Likelihood Estimation': '66',\n",
      "                                   '2.4.2 Bias, Variance and Consistency': '68',\n",
      "                                   '2.4.3 A Bayesian Approach': '71',\n",
      "                                   '2.4.4 An Example': '75',\n",
      "                                   '2.5 Sampling': '77',\n",
      "                                   '2.5.1 Inverse Transformation': '78',\n",
      "                                   '2.5.2 Rejection Sampler': '82',\n",
      "                                   '3 Optimization': '91',\n",
      "                                   '3.1 Preliminaries': '91',\n",
      "                                   '3.1.1 Convex Sets': '92',\n",
      "                                   '3.1.2 Convex Functions': '92',\n",
      "                                   '3.1.3 Subgradients': '96',\n",
      "                                   '3.1.4 Strongly Convex Functions': '97',\n",
      "                                   '3.1.5 Convex Functions with Lipschitz Continous Gradient': '98',\n",
      "                                   '3.1.6 Fenchel Duality': '98',\n",
      "                                   '3.1.7 Bregman Divergence': '100',\n",
      "                                   '3.2 Unconstrained Smooth Convex Minimization': '102',\n",
      "                                   '3.2.1 Minimizing a One-Dimensional Convex Function': '102',\n",
      "                                   '3.2.2 Coordinate Descent': '104',\n",
      "                                   '3.2.3 Gradient Descent': '104',\n",
      "                                   '3.2.4 Mirror Descent': '108',\n",
      "                                   '3.2.5 Conjugate Gradient': '111'}},\n",
      "    {   'file_name': 'thebook.pdf',\n",
      "        'matching_substrings': [   {   '3.2.6 Higher Order Methods': '115',\n",
      "                                       '3.2.7 Bundle Methods': '121',\n",
      "                                       '3.3 Constrained Optimization': '125',\n",
      "                                       '3.3.1 Projection Based Methods': '125',\n",
      "                                       '3.3.2 Lagrange Duality': '127',\n",
      "                                       '3.3.3 Linear and Quadratic Programs': '131',\n",
      "                                       '3.4 Stochastic Optimization': '135',\n",
      "                                       '3.4.1 Stochastic Gradient Descent': '136',\n",
      "                                       '3.5 Nonconvex Optimization': '137',\n",
      "                                       '3.5.1 Concave-Convex Procedure': '137',\n",
      "                                       '3.6 Some Practical Advice': '139',\n",
      "                                       '4 Online Learning and Boosting': '143',\n",
      "                                       '4.1 Halving Algorithm': '143',\n",
      "                                       '4.2 Weighted Majority': '144',\n",
      "                                       '5 Conditional Densities': '149',\n",
      "                                       '5.1 Logistic Regression': '150',\n",
      "                                       '5.2 Regression': '151',\n",
      "                                       '5.2.1 Conditionally Normal Models': '151',\n",
      "                                       '5.2.2 Posterior Distribution': '151',\n",
      "                                       '5.2.3 Heteroscedastic Estimation': '151',\n",
      "                                       '5.3 Multiclass Classification': '151',\n",
      "                                       '5.3.1 Conditionally Multinomial Models': '151',\n",
      "                                       '5.4 What is a CRF?': '152',\n",
      "                                       '5.4.1 Linear Chain CRFs': '152',\n",
      "                                       '5.4.2 Higher Order CRFs': '152',\n",
      "                                       '5.4.3 Kernelized CRFs': '152',\n",
      "                                       '5.5 Optimization Strategies': '152',\n",
      "                                       '5.5.1 Getting Started': '152',\n",
      "                                       '5.5.2 Optimization Algorithms': '152',\n",
      "                                       '5.5.3 Handling Higher order CRFs': '152',\n",
      "                                       '5.6 Hidden Markov Models': '153',\n",
      "                                       '5.7 Further Reading': '153',\n",
      "                                       '5.7.1 Optimization': '153',\n",
      "                                       '6 Kernels and Function Spaces': '155',\n",
      "                                       '6.1 The Basics': '155',\n",
      "                                       '6.1.1 Examples': '156',\n",
      "                                       '6.2 Kernels': '161',\n",
      "                                       '6.2.1 Feature Maps': '161',\n",
      "                                       '6.2.2 The Kernel Trick': '161',\n",
      "                                       '6.2.3 Examples of Kernels': '161',\n",
      "                                       '6.3 Algorithms': '161',\n",
      "                                       '6.3.1 Kernel Perceptron': '161',\n",
      "                                       '6.3.2 Trivial Classifier': '161',\n",
      "                                       '6.3.3 Kernel Principal Component Analysis': '161',\n",
      "                                       '6.4 Reproducing Kernel Hilbert Spaces': '161',\n",
      "                                       '6.4.1 Hilbert Spaces': '163',\n",
      "                                       '6.4.2 Theoretical Properties': '163',\n",
      "                                       '6.4.3 Regularization': '163',\n",
      "                                       '6.5 Banach Spaces': '164',\n",
      "                                       '6.5.1 Properties': '164',\n",
      "                                       '6.5.2 Norms and Convex Sets': '164',\n",
      "                                       '7 Linear Models': '165',\n",
      "                                       '7.1 Support Vector Classification': '165'}]}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4, sort_dicts= False)\n",
    "pp.pprint(all_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
